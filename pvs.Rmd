---
title: "The New Physical Volume Store"
subtitle: "...and how not to write unreadable files"
author: "Adam Lyon (FNAL)"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  html_document:
    highlight: pygments
    number_sections: yes
    theme: readable
abstract: >
  This document details the changes to the Physical Volume Store and releated classes in `artg4`. These changes are discussed as well as the schema evolution necessary to maintain backwards compatibility with old files. Aggregation code for reading a file that was produced by merging input files will be shown. Finally, there are some nuances with merging files that must be respected -- event numbers must not overlap. That problem is explained and some solutions are given.  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

The `gm2ringsim` Geant simulation for the Muon g-2 apparatus consists of several thousand volumes, each with a name. Some of the names are rather long, like `VacuumChamberCadMeshWallPV[06]` and `SingleStrawPV - 18 ~ 4 ~ 1 ~ 1 ~ 14`. Many of our hit and track data products record the volume that was involved in the particular interaction. Such information is useful in many studies, such as determining the effects of materials. One could save the volume name in these data products, but that would make their memory and disk footprint large. Furthermore, since many volumes are hit many times, it means that some strings would be repeated often. Root compression likely keeps the disk footprint small, but once loaded into memory the memory needed could get to be large. An obvious solution is to assign an identification label (preferably in a small number of bytes) to each volume name and store the ID value in the hit and track data products. There must also be a mapping table stored somewhere in order to translate the IDs back into the strings for analysis. This table is called the _Physical Volume Store_. 

# Old solution

For `gm2 v7_05_00` and earlier, the solution was to have a 32 bit `int` (signed, for no good reason) count up from zero for the ID number. At a basic level, the user requests an ID by passing in the Geant volume to a service's function. The service then,

1. Asks the Geant volume for its name
2. Looks up the name in the `ids` `std::vector`. If that name exists in the vector, then return the index of that string in the vector as the ID number. 
3. If the name does not exist, push back the name into the vector and return the new index for that name as the ID number.
4. The calling code then writes the ID number into the data product to represent the volume name. 

The ID to volume name table is simply a vector of strings. The index of the string is the ID number that gets stored in the hit and track data products. For using the IDs, there is a service with a function that can take the ID number and look up the string from the vector. This vector is stored into the _art_ Run record. See below for the relevant code.

## Problems with the old solution

There are several problems with this old solution that uses a vector index as the ID number to represent Geant volume name. 

* The ID number values are dependent on the order of ID number requests. For example, the first Geant volume name gets ID of 0. The second distinct volume name will get ID of 1, and so on. 
* Because the ID to volume name table depends on the order of that the Geant volumes are presented, the table in different files will be different (e.g. a volume in file A will have a different ID number than the same volume has in volume B).
* Aggregation of the table from the same run, subrun in different files is impossible.

The latter point is a major problem. 

## Old solution C++ code

The C++ code consists of several key parts (we'll look at )

There is a `artg4::PhysicalVolumeStoreService` that deals with storing the ID to volume name string table. It is also responsible for determining IDs and adding new ones to the table. See below and [here](https://redmine.fnal.gov/redmine/projects/artg4/repository/revisions/33d5/entry/pluginActions/physicalVolumeStore/physicalVolumeStore_service.hh),

```c++
// artg4/pluginActions/physicalVolumeStore/physicalVolumeStoreService.hh  (partial listing)
namespace artg4 {

  class PhysicalVolumeStoreService : public artg4::RunActionBase {
    
    public:
      PhysicalVolumeStoreService(fhicl::ParameterSet const&, art::ActivityRegistry&);
      virtual ~PhysicalVolumeStoreService();
    
      // Prepare Art for our data
      virtual void callArtProduces(art::EDProducer * producer);
    
      // Get the UID and add to the map
      unsigned int idGivenPhysicalVolume(const G4VPhysicalVolume* pv);

      // Write out our data to the Run record
      virtual void fillRunEndWithArtStuff(art::Run& r);
    
    private:
    
      // The map
      std::unique_ptr<artg4::PhysicalVolumeStoreData> pvs_;
      
      // ...
```

The important part of the .cc file is (see [here](https://redmine.fnal.gov/redmine/projects/artg4/repository/revisions/33d5/entry/pluginActions/physicalVolumeStore/physicalVolumeStore_service.cc) for the whole thing),

```c++
// artg4/pluginActions/physicalVolumeStore/physicalVolumeStoreService.cc  (partial listing)
unsigned int artg4::PhysicalVolumeStoreService::idGivenPhysicalVolume(
                                            const G4VPhysicalVolume* pvptr ) {
  
  // Determine the id
  return pvs_->idGivenString( pvptr->GetName() );
}
```
As seen above, users call `artg4::idGivenPhysicalVolme` first gets the name from the volume and passes that to the physical volume store data object. Let's look at that code (see [here](https://redmine.fnal.gov/redmine/projects/artg4/repository/revisions/33d5/entry/pluginActions/physicalVolumeStore/PhysicalVolumeStoreData.hh) )

```c++
// artg4/pluginActions/physicalVolumeStore/PhysicalVolumeStoreData.hh  (partial listing)
namespace artg4 {
  class PhysicalVolumeStoreData {
    public:
      // ...
    
      // Given a string, return the ID
      unsigned int idGivenString(const std::string & s) { return ids_.idGivenString(s); }

      // Given the ID, return the string (you'll call this most often when reading)
      const std::string & stringGivenID(unsigned int id) const { return ids_.stringGivenID(id); }
    
      // The number of entries
      unsigned int size() const { return ids_.size(); }
      
    private:
      StringIDs ids_;
    
    // ...
```

Much of this code forwards to `artg4::StringIDs` - where the real work happens. Looking at that code (see [here](https://cdcvs.fnal.gov/redmine/projects/artg4/repository/revisions/33d5/entry/util/StringIDs.hh) and [here](https://cdcvs.fnal.gov/redmine/projects/artg4/repository/revisions/33d5/entry/util/StringIDs.cc))

```c++
// artg4/pluginActions/util/StringIDs.hh  (partial listing)
namespace artg4 {
    class StringIDs {
    public:
      // ...

      // Given a string, return the ID and add it to the list
      unsigned int idGivenString(const std::string & s);

      // Given an ID, return the string
      const std::string & stringGivenID(unsigned int id) const {
        return stringVec_.at( id );
      }
    
      // Return the number of entries
      unsigned int size() const { return stringVec_.size(); }
    
    private:
    
      // The vector that holds the strings
      std::vector<std::string> stringVec_;
    
      // Auxillary map
      std::map<std::string, unsigned int> stringToIdMap_;
      
      // ...
```

And the implementation file...

```c++
// artg4/pluginActions/physicalVolumeStore/PhysicalVolumeStoreData.cc  (partial listing)
// ...

// Get the ID for a string. If it is not already in the vector, then add it
unsigned int artg4::StringIDs::idGivenString( const std::string & s ) {
  // Do we already have s?
  unsigned int val = 0;
  bool found = false;
  
  // If the string vector is not empty, then we have to look in the map
  if ( ! stringVec_.empty() ) {
    
    // Look in the map for the ID number
    auto mapIter = stringToIdMap_.find(s);
    if ( mapIter != stringToIdMap_.end() ) {
      // Found it
      val = mapIter->second;
      found = true;
    }
  }
  
  if ( ! found ) {
    // String not found, add it
    val = stringVec_.size();
    stringVec_.push_back( s );
    stringToIdMap_[ s ] = val;
  }
  
  return val;
}

// ...
```

When `idGivenString` is called with a string representing the volume name, the code checks to see if that string is already known (already in the physical volume store). A map is used to aid in this search. If the string is found, then the corresponding ID is looked up and returned. If the string is not found, then it is new and added both to the end of the vector and to the map. The ID value is simply the size of the vector before the string was added (e.g. the index into the vector). That ID value is then returned and stored in the hit and track objects.

When one has a hit or track data product with the volume ID, eventually `artg4::StringIDs::stringGivenID` is called. As seen in the header file, that simply returns the contents of the vector at the index given by the ID. 

This method is quite simple, but has problems as mentioned above.  

# New solution

A new PhysicalVolumeStore is necessary to fix the deficiencies of the old design, and it will likely be included in the next release (`gm2 v7_06_00`). The main problem with the old design is that the ID to volume name table is not consistent from file to file. That is a volume in one file may have a different ID compared to that same volume in another file. The solution is to abandon the simple vector of volume names and the use of its index. The replacement is determining a _hash_ value for the volume name string and then storing a map that links the hash values and the string. A [hash function](https://en.wikipedia.org/wiki/Hash_function) maps arbitrary sized data (e.g. volume name strings) to data with fixed size (the hash values or IDs in this case). The hash function is deterministic in that for a given string, the same ID is always returned. This feature solves the problems of the old solution, as now a particular volume string always corresponds to the same ID value in every file. 

There are many possible hash functions, including [crc](https://en.wikipedia.org/wiki/Cyclic_redundancy_check) and [md5](https://en.wikipedia.org/wiki/MD5). The former returns 32 bit hash values and the latter 128 bit. The larger sized hash values have a lower chance of a hash _collision_. A collision occurs when, for our case, two different volume name strings resolve to the same hash value. See this nice [article](http://preshing.com/20110504/hash-collision-probabilities/) on this subject. We have over 2000 volumes in our Geant description. With a 32 bit hash, the probability of a collision is around 0.1%. While that may seem low, a collision basically breaks everything. For a 64 bit hash, the chance of a collision is between one in a trillion and one in ten trillion - acceptable! C++14 provides an object that can return a 64 bit hash that we will use: `std::hash`. See this [article](http://en.cppreference.com/w/cpp/utility/hash) for more information. So the new workflow is,

1. Given a Geant volume name string, its 64 bit hash value (ID) is determined with `std::hash`. 
2. The ID is then looked up in an std::map (key is the ID and value is the string).
3. If the ID is not in the map, then the (ID, volume name string) pair is added.
4. If the ID is found in the map, then the corresponding string is checked to ensure it is identical to the string passed in. If they are not identical, then we have a hash collision and all hope is lost. 
5. The ID is returned to the calling function to be written into a data product. 

The `std::hash` function returns a hash of type `size_t`. `size_t` is an integer value whose number of bits is what is natural for the machine. In all of our cases, that size is 64 bits or eight bytes. 

When using a volume ID in a data product, that hash is simply looked up in the map and the corresponding string is returned as the volume name. 

## New solution C++ code

The only code with appreciable changes are in `StringID`. 

```c++
// artg4/pluginActions/util/StringIDs.hh  (partial listing)
namespace artg4 {
  class StringIDs {
    public:
    
      // ...
      
      // Given a string, return the ID and add it to the list
      size_t idGivenString(const std::string & s);

      // Given an ID, return the string
      const std::string & stringGivenID(size_t id) const {
        return hashToStringMap_.at( id );
      }

    private:
    
      // The map that holds the strings
      std::map< size_t, std::string> hashToStringMap_;

      // Don't expose the hash to ROOT
      #ifndef __ROOTCLING__
      std::hash<std::string> hash_fn_;
      #endif
      
    // ...
```

The vector has been replaced with a map with the hash (`size_t`) as the key and the string as the value. `artg4::StringIDs::stringGivenID` looks up the ID (hash) in the map and returns the corresponding string. The implementation file has the code for adding an ID to the map. 

```c++
// artg4/pluginActions/util/StringIDs.cc  (partial listing)
// Get the ID for a string. If it is not already in the vector, then add it
size_t artg4::StringIDs::idGivenString( const std::string & s ) {

  // Form the hash
  size_t id = hash_fn_(s);

  // Does this hash already exist in the map?
  if ( hashToStringMap_.find(id) != hashToStringMap_.end() ) {

    // Check for hash collision (the hashed string does not equal our string)
    if ( hashToStringMap_[id] != s ) {
      // HASH COLLISION !!
      throw cet::exception("artg4::StringIDs")
          << "Hash collision detected!";
    }
  }
  else {

   // Add to the map
   hashToStringMap_.emplace(id, s);
  }

  return id;
}
```

This function, `artg4::StringIDs::idGivenString` follows the workflow discussed above. This version of `PhysicalVolumeStore` will give the same 64 bit IDs for the same Geant volumes across all files. 

## Schema evolution

It is important that code be backwards compatible with old data files. Art, using ROOT's schema evolution system, can transparently handle minor changes in data products from version to version. For example, if a new version of a data product has a new data memmber, old data files without that data member can still be read in; the missing data member will just have random data. If a data member type changes and it is possible to coerce the old type into the new type then this coercision will happen automatically for old files. 

Files with the old Physical Volume Store should be readable with the new code. Fortunately, there is no requirement to turn the old IDs into hashes, as doing so would be extremely difficult. We do, however, want the old poor IDs to be used for lookups to get the volume name strings. 

The new `artg4::StringIDs` has much different internals than the old version of the class. The hit and track data products in old data files that store `int` for the ID will silently be coerced into `size_t` for the new code. But there can be no automatic conversion for the `artg4::StringIDs` themselves. Therefore, schema evolution code is written in `classes_def.xml`, as shown below.

```c++
<!-- classes_`def.xml -->
<lcgdict>
    <class name="artg4::StringIDs"   ClassVersion="15">
        <version ClassVersion="15"  checksum="1375741644"/>
        <version ClassVersion="14"  checksum="2137085475"/>
    </class>
    <class name="std::map<size_t, std::string>"/>
    <class name="art::Wrapper<artg4::StringIDs>"/>

    <!-- Schema evolution rules for artg4::StringIDs -->
    <!-- version 14 -> 15 -->
    <ioread
        version="[-14]"
        sourceClass="artg4::StringIDs"
        source="std::vector<std::string> stringVec_"
        targetClass="artg4::StringIDs"
        target="hashToStringMap_"
        include="vector;string;map">
        <![CDATA[
           // Convert old style integer StringIDs to new hash type - but don't generate a hash
           // as there is no way to transmit the hashes to the hit classes with the integers
           std::cout << "WARNING: Converting StringIDs to new style but not using hashes " << 
                                                "- so will not be mergable" << std::endl;
           for ( unsigned int i = 0; i < onfile.stringVec_.size() ; ++i ) {
             hashToStringMap_[ static_cast<size_t>(i) ] = onfile.stringVec_[i];
           }
        ]]>
    </ioread>
```

See [ROOT's documentation](https://root.cern.ch/root/html/io/DataModelEvolution.html) on schema evolution as well as a nice [example](https://redmine.fnal.gov/redmine/projects/lardataobj/repository/revisions/develop/entry/lardataobj/RecoBase/classes_def.xml) from LArSoft. 

The first thing to note is that version identifiers must be assigned and associated with a checksum. The old version had no identifier, so it will be called version 14. The new version will be caled 15. To determine the checksum, you must set up the execution environment for a particular version (e.g. `setup gm2 v7_05_00` for old and `setup gm2 v7_06_00` for new). You then tell ROOT to load a data file with the objects in question. You can then ask ROOT for the checksum (see below). Note that ROOT will look up the class definition __from the environment__, not from the file (so if you are in the new `v7_06_00` and load an old file, you will get a checksum corresponding to the new class). Here is the code

```bash
#!/bin/bash
root -b -l -q aFile.art checksum.C
```

```c++
// checksum.C
{
  std::cout << TClass::GetClass("artg4::StringIDs")->GetCheckSum() << std::endl;
}
```

We can then follow the schema evolution section within the `<ioread>` tags in `classes_def.xml` above. Items within this tag will be used to create C++ code that ROOT will run when the old version of `artg4::StringIDs` is read in to convert those objects to the new class schema. Here are details about each item. See the ROOT documentation for more information.

* `version="[-14]"` means this schema evolution code is for versions of `artg4::StringIDs` version 14 and earlier.
* `sourceClass="artg4::StringIDs"` identifies the class to evolve
* `source="std::vector<std::string> stringVec_"` identifies member data of the old class that will be involved in the evolution. Because this member datum is not in the new class, we must specify the type.
* `targetClass="artg4::StringIDs"` identifies the destination class of the evolution. In this case, it is the same class 
* `target="hashToStringMap_"` identifies the destination member data of the evolution. Note that we do not need to give the type as ROOT can figure it out from the environment. 
* `include=...` Include files the C++ needs, e.g. `#include <vector>`
* The code is written within the `CDATA` (character data) block. `onfile.X` refers to member `X` of the old version class (the version that is in the file). Here we see the strings from the old `stringVec_` are copied into the new `hashToStringMap_`. So the vector index becomes the hash for lookups (remember, we are not converting to an actual 64 bit hash). 

This schema evolution code will allow old files to be read in by the new code and the volume ID lookups will work. But again - for old files, volume IDs for the same volume name will be different for different files. New Monte Carlo files will need to be produced to take advantage of the new features of the hashed volume names. 

## Aggregation

The new Physical Volume Store solution allows for objects to be aggregated. Aggregation happens when events from the same run are written across more than one file. See [art's documentation](https://cdcvs.fnal.gov/redmine/projects/art/wiki/Run_and_SubRun_products) for more information. A _Run record_ is always written out at the end of the file, regardless of whether or not the run realy ends there. If you then read in those files from that run and write out _one_ output file, _art_ must figure out how to merge (or aggregate) the run data objects for the "merged" output file. _art_ can do some automatic aggregation (see the documentation mentioned above), but it cannot for the physical volume store. Therefore, aggregation code is written into `PhysicalVolumeStoreData`, shown here...

```c++
namespace artg4 {
  class PhysicalVolumeStoreData {
    public:
      // ...
    
      // aggregate
      void aggregate(PhysicalVolumeStoreData const & other ) {
            for ( auto const& anID : other.ids() ) {
                  idGivenString( other.stringGivenID(anID));
            }
      }
    
      // Get a vector of ids
      std::vector<size_t> ids() const { return ids_.ids(); }
      
    private:
      StringIDs ids_;
// ...
```

The `aggregate` method simply loops over the hashes in the Physical Volume Store from the other run (`other`) and adds them to the current run Physical Volume Store by calling `idGivenString` (the returning hash is not needed). This code will also check for hash collisions. Note that the `ids` method is shown and simply returns a `std::vector` of the hash values from the map. 

# Event number overlap ruins files

The Aggregation section above mentions spreading events from the same run over many files. This situation can happen when you launch run several Monte Carlo jobs producing output with the same run and subrun numbers. If this happens, then the event numbers in the files __MUST NOT OVERLAP__. That is no event number can be repeated across all of the files that have the same run and subrun numbers. That is, 

|File|Run|Subrun|Event range|
|----|---|------|-----------|
| A  | 1 | 505  |  1-999    |
| B  | 1 | 505  |  1-999    |

If you produce a file `C` that is the output from running over `A` and `B` _in the same job_ (so 2 files are reduced to 1), that file `C` will be __unreadable__ by _art_ as you will get a overlapping events exception thrown. There is no way to recover from this problem. You can still analyze files `A` and `B`, but any output _art_ file that is merging will be useless. 

There are two solutions. 

## Non-overlapping events solution

When you make file `B`, in the FCL set `source.firstEvent : 1000 ` so that the events will not overlap. For example, 

|File|Run|Subrun|Event range|
|----|---|------|-----------|
| A  | 1 | 505  |  1-999    |
| B  | 1 | 505  |  1000-1999 |

You will need to know the event range of file `A` to know how to set the FCL file when you produce file `B`. Sometimes, it can be problematic to get that information. 

## Different subruns solution

Specifying differnet subrun numbers will also solve this problem. That is, 

|File|Run|Subrun|Event range|
|----|---|------|-----------|
| A  | 1 | 505  |  1-999    |
| B  | 1 | 506  |  1-999    |

This solution can be complicated if there are subrun data products. So far Muon g-2 does not have subrun data products. 

As a general rule, it is a good idea to produce all Monte Carlo files with different subrun numbers. Here is an easy way to do that automatically. We assume that `myFCL.fcl` is the fcl file you want to run...

```bash
#!/bin/bash
t=$(mktemp /tmp/myFCL_XXXX.fcl) ; R=$(date +%s) ; cp myFCL.fcl $t ; \\ 
        echo "source.firstSubRun : $R" >> $t ;  gm2 -c $t
# Note that you can add more options to gm2 like "gm2 -c $t -n 500 -o output.art"
```

This code makes a temporary file. It will also use the date and time in integer form (seconds since unix epoch) as the subrun number. The FCL file to run is copied to the tmp area and is altered to set the subrun number. Then production begins using this new FCL file. Every job will get its own subrun number. You can choose other things other than `date`, like `R=$RANDOM` for a truely random integer. 

